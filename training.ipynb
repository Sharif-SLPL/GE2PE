{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-22T11:32:25.528526Z","iopub.status.busy":"2022-06-22T11:32:25.527821Z","iopub.status.idle":"2022-06-22T11:33:00.219537Z","shell.execute_reply":"2022-06-22T11:33:00.218536Z","shell.execute_reply.started":"2022-06-22T11:32:25.528431Z"},"trusted":true},"outputs":[],"source":["!pip install gdown"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-22T11:33:00.223713Z","iopub.status.busy":"2022-06-22T11:33:00.223416Z","iopub.status.idle":"2022-06-22T11:33:26.259101Z","shell.execute_reply":"2022-06-22T11:33:26.257799Z","shell.execute_reply.started":"2022-06-22T11:33:00.223682Z"},"trusted":true},"outputs":[],"source":["! gdown data_link #data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-22T11:33:26.263303Z","iopub.status.busy":"2022-06-22T11:33:26.260779Z","iopub.status.idle":"2022-06-22T11:33:35.868829Z","shell.execute_reply":"2022-06-22T11:33:35.867812Z","shell.execute_reply.started":"2022-06-22T11:33:26.263257Z"},"trusted":true},"outputs":[],"source":["!pip install transformers\n","!pip install datasets\n","!pip install jiwer\n","!pip install torch"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-22T11:34:06.21536Z","iopub.status.busy":"2022-06-22T11:34:06.214968Z","iopub.status.idle":"2022-06-22T11:34:13.594374Z","shell.execute_reply":"2022-06-22T11:34:13.593551Z","shell.execute_reply.started":"2022-06-22T11:34:06.215316Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from datasets import Dataset\n","import argparse\n","import torch\n","\n","import os\n","from datasets import load_metric\n","from dataclasses import dataclass\n","from typing import Union, Dict, List, Optional\n","from transformers import AdamW, AutoTokenizer, T5ForConditionalGeneration, T5Config\n","from transformers import (\n","    DataCollator,\n","    Seq2SeqTrainer, \n","    Seq2SeqTrainingArguments,\n","    set_seed,\n",")\n","\n","\n","def load_pronuncation_dictionary(train = True) -> Dataset:\n","    path = '/kaggle/working/FD.csv'\n","    df = pd.read_csv(path, index_col = [0]).dropna()\n","    Plen = np.array([len(i) for i in df['Phoneme']])\n","    df = df.iloc[Plen < 512, :]\n","    df = df.sample(frac = 1)\n","    # 10 percent of FarsDat data is 90.\n","    if train:\n","        return Dataset.from_pandas(df.iloc[:len(df)-90,:])\n","    else:\n","        return Dataset.from_pandas(df.iloc[len(df)-90:,:])\n","\n","\n","def prepare_dataset(batch):\n","    \n","    batch['input_ids'] = batch['word']\n","    batch['labels'] = batch['pron']\n","    \n","    return batch\n","    \n","\n","@dataclass\n","class DataCollatorWithPadding:\n","\n","    tokenizer: AutoTokenizer\n","    padding: Union[bool, str] = True\n","\n","    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n","        # split inputs and labels since they have to be of different lenghts and need\n","        # different padding methods\n","        words = [feature[\"input_ids\"] for feature in features]\n","        prons = [feature[\"labels\"] for feature in features]\n","\n","        batch = self.tokenizer(words,padding=self.padding,add_special_tokens=False,\n","                          return_attention_mask=True,return_tensors='pt')\n","        pron_batch = self.tokenizer(prons,padding=self.padding,add_special_tokens=True,\n","                          return_attention_mask=True,return_tensors='pt')\n","        \n","        # replace padding with -100 to ignore loss correctly\n","        batch['labels'] = pron_batch['input_ids'].masked_fill(pron_batch.attention_mask.ne(1), -100)\n","\n","\n","        return batch\n","    \n","                \n","\n","\n","def compute_metrics(pred):\n","    labels_ids = pred.label_ids\n","    pred_ids = pred.predictions\n","\n","    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n","    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n","\n","    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n","    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n","    return {\"cer\": cer, 'wer':wer}\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-22T11:34:13.596443Z","iopub.status.busy":"2022-06-22T11:34:13.595776Z","iopub.status.idle":"2022-06-22T11:34:16.957478Z","shell.execute_reply":"2022-06-22T11:34:16.956539Z","shell.execute_reply.started":"2022-06-22T11:34:13.596405Z"},"trusted":true},"outputs":[],"source":["import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-22T11:34:16.959489Z","iopub.status.busy":"2022-06-22T11:34:16.958947Z"},"trusted":true},"outputs":[],"source":["parser = argparse.ArgumentParser()\n","parser.add_argument('--data', type=str, default=None)\n","parser.add_argument('--model',type=str,default='byt5')\n","parser.add_argument('--model_name',type=str,default='google/byt5-small')\n","parser.add_argument('--train_data',type=str,default='data/train')\n","parser.add_argument('--dev_data',type=str,default='data/dev')\n","parser.add_argument('--test_data',type=str,default='data/test')\n","parser.add_argument('--pretrained_model', type=bool, default=False)\n","parser.add_argument('--output_dir',type=str,default='./')\n","parser.add_argument('--train',action='store_true', default = True)\n","parser.add_argument('--evaluate',action='store_true')\n","parser.add_argument('--checkpoint',default=None,type=str)\n","parser.add_argument('--resume_from_checkpoint',action='store_true')\n","\n","\n","# trainign hyperparameters\n","parser.add_argument('--fp16',type=bool,default=False,help=\"fp16 not available for switch transformers\")\n","parser.add_argument('--train_batch_size',type=int,default=25)\n","parser.add_argument('--learning_rate',type=float,default=5e-4)\n","parser.add_argument('--warmup_steps',type=int,default=1000)\n","parser.add_argument('--epochs',type=int,default=20)\n","parser.add_argument('--eval_batch_size',type=int,default=100)\n","parser.add_argument('--gradient_accumulation',type=int,default=2)\n","parser.add_argument('--logging_steps',type=int,default=1000)\n","parser.add_argument('--save_steps',type=int,default=4000)\n","parser.add_argument('--eval_steps',type=int,default=1000)\n","parser.add_argument('--unk_prob',type=float,default=0.85)\n","\n","# model hyperparameters\n","parser.add_argument('--num_encoder_layers',type=int,default=2)\n","parser.add_argument('--num_decoder_layers',type=int,default=2)\n","parser.add_argument('--d_model',type=int,default=512)\n","parser.add_argument('--d_kv',type=int,default=64)\n","parser.add_argument('--d_ff',type=int,default=512)\n","\n","# MoE hyperparameters\n","parser.add_argument('--capacity_factor',type=float,default=1.0)\n","parser.add_argument('--n_experts',type=int,default=8)\n","parser.add_argument('--load_balancing_loss_weight',type=float,default=1e-2)\n","parser.add_argument('--is_scale_prob',type=bool,default=True)\n","parser.add_argument('--drop_tokens',type=bool,default=False)\n","\n","args = parser.parse_args(args=[])\n","\n","# setting the evaluation metrics\n","cer_metric = load_metric(\"cer\")\n","wer_metric = load_metric('wer')\n","\n","if args.train == True:\n","\n","    # loading and preprocessing data\n","    train_data = load_pronuncation_dictionary(True)\n","    train_data = train_data.map(prepare_dataset)    \n","    train_dataset = train_data\n","\n","    dev_data = load_pronuncation_dictionary(False)\n","    dev_data = dev_data.map(prepare_dataset)    \n","    dev_dataset = dev_data\n","\n","    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n","    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","    # intitalizing the model\n","    if args.pretrained_model == True:\n","        print('Loading pretrained model...')\n","        model = T5ForConditionalGeneration.from_pretrained(args.model_name)\n","    else:\n","\n","        config = T5Config.from_pretrained(args.model_name)\n","\n","        config.num_decoder_layers = args.num_decoder_layers\n","        config.num_layers = args.num_encoder_layers\n","        config.d_kv = args.d_kv\n","        config.d_model = args.d_model\n","        config.d_ff = args.d_ff\n","\n","        print('Initializing a ByT5 model...')\n","        model = T5ForConditionalGeneration(config)\n","\n","    training_args = Seq2SeqTrainingArguments(\n","        predict_with_generate=True,\n","        generation_num_beams=5,\n","        generation_max_length=512,\n","        evaluation_strategy=\"steps\",\n","        per_device_train_batch_size=args.train_batch_size,\n","        per_device_eval_batch_size=args.eval_batch_size,\n","        num_train_epochs=args.epochs,\n","        gradient_accumulation_steps=args.gradient_accumulation,\n","        learning_rate=args.learning_rate,\n","        warmup_steps=args.warmup_steps,\n","        lr_scheduler_type=\"cosine\",\n","        fp16=args.fp16, \n","        output_dir=args.output_dir,\n","        logging_steps=args.logging_steps,\n","        save_steps=args.save_steps,\n","        eval_steps=args.eval_steps,\n","        save_total_limit=2,\n","        load_best_model_at_end=True\n","    )\n","\n","\n","    trainer = Seq2SeqTrainer(\n","        model=model,\n","        tokenizer=tokenizer,\n","        args=training_args,\n","        compute_metrics=compute_metrics,\n","        train_dataset=train_dataset,\n","        eval_dataset=dev_dataset,\n","        data_collator=data_collator,\n","    )\n","\n","    if args.resume_from_checkpoint:\n","        trainer.train(resume_from_checkpoint=True)\n","    else:\n","        trainer.train()\n","\n","    trainer.save_model(args.output_dir)\n","\n","\n","elif args.evaluate == True:\n","    test_data = load_pronuncation_dictionary(True)\n","    test_data = test_data.map(prepare_dataset)    \n","    test_dataset = test_data    \n","\n","\n","\n","    tokenizer = AutoTokenizer.from_pretrained(args.checkpoint)\n","    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","    model = T5ForConditionalGeneration.from_pretrained(args.checkpoint)\n","\n","\n","    training_args = Seq2SeqTrainingArguments(\n","        predict_with_generate=True,\n","        generation_num_beams=5,\n","        evaluation_strategy=\"steps\",\n","        per_device_train_batch_size=args.train_batch_size,\n","        per_device_eval_batch_size=args.eval_batch_size,\n","        output_dir=args.output_dir,\n","        logging_steps=args.logging_steps,\n","        save_steps=args.save_steps,\n","        eval_steps=args.eval_steps,\n","        save_total_limit=2\n","    )\n","\n","\n","    trainer = Seq2SeqTrainer(\n","        model=model,\n","        tokenizer=tokenizer,\n","        args=training_args,\n","        compute_metrics=compute_metrics,\n","        data_collator=data_collator\n","    )\n","\n","\n","\n","    eval_results = trainer.evaluate(eval_dataset=test_dataset,num_beams=5)\n","    print(eval_results)\n","    with open(os.path.join(args.output_dir, 'results'),'w') as out:\n","        out.write('%s\\t%s\\t%s\\n'%(args.language,eval_results['eval_cer'],eval_results['eval_wer']))\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
